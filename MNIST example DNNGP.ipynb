{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network GP MNIST example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a demonstration of the DNNGP implementation from Google Brain paper found here https://arxiv.org/abs/1711.00165\n",
    "\n",
    "This implementation was done for the sake of research and comparison to a linear approximation of the recurrence relation. More information about the approximation can be found here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mlxtend.data import mnist_data\n",
    "import sklearn.preprocessing as skpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixed_mnist(count):\n",
    "    total = count*10\n",
    "    valid = int(np.floor(count/10))\n",
    "    rawset = mnist_data()\n",
    "    arranged_data = np.zeros((1,len(rawset[0][0])))\n",
    "    arranged_target = np.zeros((1,1))\n",
    "    for i in range(10):\n",
    "        arranged_data = np.append(arranged_data,rawset[0][500*i:(500*i)+count],axis=0)\n",
    "        arranged_target = np.append(arranged_target,np.ones((count,),dtype=int)*i)\n",
    "    for i in range(10): #validation\n",
    "        arranged_data = np.append(arranged_data,rawset[0][500*(i+1)-valid:500*(i+1)],axis=0)\n",
    "        arranged_target = np.append(arranged_target,np.ones((valid,),dtype=int)*i)\n",
    "    arranged_data = np.delete(arranged_data,0,axis=0)\n",
    "    arranged_target = np.delete(arranged_target,0,axis=0)\n",
    "    return [arranged_data,arranged_target,valid*10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = mixed_mnist(100)\n",
    "min_max_scaler = skpp.MinMaxScaler()\n",
    "x_train_minmax = min_max_scaler.fit_transform(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a one hot encoder. Inputs have been scaled from 0->1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = skpp.OneHotEncoder()\n",
    "enc.fit(data[1].reshape(-1,1))\n",
    "enc.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = len(enc.transform([[1]]).toarray()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.transform(data[1][0].reshape(-1,1)).toarray()[0]\n",
    "len(data[1][:-data[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.zeros((len(data[0])-data[2],len(data[0][0])+classes))\n",
    "X_test = np.zeros((data[2],len(data[0][0])+classes)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data[0][:-data[2]])):\n",
    "    X_train[i] = np.append(data[0][i],enc.transform(data[1][i].reshape(-1,1)).toarray()[0])\n",
    "for i in range(data[2]):\n",
    "    X_test[i] = np.append(data[0][-(i+1)],np.ones(classes)*(1/classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test and train vectors have been appropriately one-hot encoded, now we want to normalize the vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_norm = skpp.normalize(X_train, norm='l2')\n",
    "X_test_norm = skpp.normalize(X_test, norm='l2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our training vectors and test vectors scaled, normalized and encoded.\n",
    "\n",
    "Next we want to make an instance of the GP and then evaluate it. All we need to do is pass it the training data, corresponding target values, test data, and then the hyperparamters sigb, sigw, and layers. Sigma b and sigma w correspond to the variance of the bias and weights respectively.\n",
    "\n",
    "Calling DNNGP.train() carries out the full evaluation. To use the approximation all that's needed to do is pass a True to the DNNGP.train() function (train(True)). The approximation uses a linear function derived from the recurrence which can speed up training dramatically. If approximation has been done there will be support for getting predictions for new test data not originally included. This will be implemented in the future. For more data on the approximation see [arxiv:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DNNGP import DNNGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sigb, sigw, layers = 0.3520467, 2.1220488, 87 #0.595, 2.26, 8\n",
    "#sigb, sigw, layers = 2.5478501548763366, 0.6811691126057303, 17\n",
    "#sigb, sigw, layers = 1.4155776,1.30368223,31\n",
    "\n",
    "#After adjusting to 2.14x + 1\n",
    "#sigb, sigw, layers = 0.75782449687, .1, 64\n",
    "#sigb,sigw,layers = 1.889194,0.8175861995,17 #Best\n",
    "#sigb, sigw, layers = 0.49613546,1.244503, 26 #All-around best logloss and absolute\n",
    "sigb,sigw,layers = 2.1835419, 2.09, 66 #softmax log-loss\n",
    "gp = DNNGP(X_train_norm,data[1][:-data[2]],X_test_norm,sigb,sigw,layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our GP has been evaluated we can access predictions in the form of raw data or taken from classification of one-hot. For this purpose we will just take the classifications directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = gp.prediction()\n",
    "predict = predict[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(predict) #Test to make sure that all of our classes are represented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_count = len(data[1][-data[2]:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we calculate our accuracy using 0-1 loss. For optimization we'd want to compute the MSE from raw counts but just for demonstration a plain accuracy should be fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.92\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for i in range(test_count):\n",
    "    if data[1][-data[2] + i] == predict[i]:\n",
    "        correct +=1\n",
    "print(correct / len(predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for comparison we will evaluate the GP with the approximate recurrence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp.train(approximate=True)\n",
    "predict = gp.prediction()\n",
    "predict = predict[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_count = len(data[1][-data[2]:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.85\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for i in range(test_count):\n",
    "    if data[1][-data[2] + i] == predict[i]:\n",
    "        correct +=1\n",
    "print(correct / len(predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with the one-shot flattened case which should provide the same performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gp.train(one_shot=True)\n",
    "predict = gp.prediction()\n",
    "predict = predict[::-1]\n",
    "np.unique(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.84\n"
     ]
    }
   ],
   "source": [
    "test_count = len(data[1][-data[2]:])\n",
    "correct = 0\n",
    "for i in range(test_count):\n",
    "    if data[1][-data[2] + i] == predict[i]:\n",
    "        correct +=1\n",
    "print(correct / len(predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using MSE and optimizing over runs into a problem that the optimal solution is making the entire vector as small as possible... Need to implement cross-entropy or reimplement MSE. Might also just try optimizing over Accuracy directly"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
